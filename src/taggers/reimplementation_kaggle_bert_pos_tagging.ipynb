{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsaha/venvs/cl_dreamscape/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.data import load as nltk_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a huggingface Dataset from the stored sentence and pos tag data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQDMBytesReader(object):\n",
    "    # For showing the progress bar while reading the stored pickle file.\n",
    "    def __init__(self, fd, **kwargs):\n",
    "        self.fd = fd\n",
    "        from tqdm import tqdm\n",
    "        self.tqdm = tqdm(**kwargs)\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        bytes = self.fd.read(size)\n",
    "        self.tqdm.update(len(bytes))\n",
    "        return bytes\n",
    "\n",
    "    def readline(self):\n",
    "        bytes = self.fd.readline()\n",
    "        self.tqdm.update(len(bytes))\n",
    "        return bytes\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.tqdm.__enter__()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        return self.tqdm.__exit__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    # if os.path.exists(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\"):\n",
    "    if os.path.exists(f\"/home/rsaha/projects/babylm/src/taggers/data/{file_name}.pkl\"):\n",
    "        print(\"Loading data from file ...\")\n",
    "        with open(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\", \"rb\") as f:\n",
    "            total = os.path.getsize(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\")\n",
    "            with TQDMBytesReader(f, total=total) as pbfd:\n",
    "                up = pickle.Unpickler(pbfd)\n",
    "                X_data, y_data = up.load()\n",
    "            return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:  ['/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_switchboard.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_open_subtitles.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_childes.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_simple_wiki.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_bnc_spoken.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_local_narr_captions.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_gutenberg.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_cc_3M_captions_reduced.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paths: 100%|██████████| 8/8 [00:00<00:00, 39429.41it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/rsaha/projects/babylm/src/taggers/data/\")\n",
    "paths = [str(f) for f in data_dir.glob(\"*\") if f.is_file() and not f.name.endswith(\".DS_Store\") and f.suffix in [\".pkl\"]]\n",
    "print(\"Paths: \", paths)\n",
    "\n",
    "file_names = []\n",
    "# Only select the cc_3m and local_narr files and store it in filtered_paths.\n",
    "# filtered_paths = paths #[]\n",
    "# # for path in paths:\n",
    "# #     if \"cc_3M\" in path or \"local_narr\" in path:\n",
    "# #         filtered_paths.append(path)\n",
    "\n",
    "for path in tqdm(paths, desc=\"Paths\"):\n",
    "\n",
    "    file_name = Path(path).name\n",
    "    # Drop the .train extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tagging_dataset_all_sentences_switchboard\n",
      "pos_tagging_dataset_all_sentences_open_subtitles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tagging_dataset_all_sentences_childes\n",
      "pos_tagging_dataset_all_sentences_simple_wiki\n",
      "pos_tagging_dataset_all_sentences_bnc_spoken\n",
      "pos_tagging_dataset_all_sentences_local_narr_captions\n",
      "pos_tagging_dataset_all_sentences_gutenberg\n",
      "pos_tagging_dataset_all_sentences_cc_3M_captions_reduced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data from each file_name in file_names.\n",
    "all_X_data = []\n",
    "all_y_data = []\n",
    "for file_name in tqdm(file_names):\n",
    "    print(file_name)\n",
    "    if file_name == \"pos_tagging_dataset_all_sentences_open_subtitles\":\n",
    "        data = pickle.load(open(f\"/home/rsaha/projects/babylm/src/taggers/data/{file_name}.pkl\", \"rb\"))\n",
    "        # all_X_data.extend(X_data)\n",
    "        # all_y_data.extend(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of numbers where each tag (the second element in the tuple) is assigned a unique number. This will be the class labels.\n",
    "tagdict = nltk_load('help/tagsets/upenn_tagset.pickle')\n",
    "label_names = {t: i for i, t in enumerate(tagdict.keys())}\n",
    "label_names['#'] = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the empty lists from data.\n",
    "data = [d for d in data if d [0]!= []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now remove all the hastags from each example of the data.\n",
    "data_no_hash = []\n",
    "data_no_hash = [d for d in data_no_hash if d [0]!= []]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_no_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732818/1732818 [00:02<00:00, 670467.92it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data has many tuples and each tuple has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "# Create a third separate list of lists where each list contains the number from the label_names dictionary based on the key tag.\n",
    "# This will be the class labels.\n",
    "tag_to_class_mapping_for_data_no_hash = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    tag_to_class_mapping_for_data_no_hash.append([label_names[tag] for tag in data_no_hash[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1732818/1732818 [00:07<00:00, 230371.83it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data has many tuples and each tuple has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "# Create a third separate list of lists where each list contains the number from the label_names dictionary based on the key tag.\n",
    "# This will be the class labels.\n",
    "tag_to_class_mapping_for_data = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    tag_to_class_mapping_for_data.append([label_names[tag] for tag in data[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe from the sentence, tags, and class labels.\n",
    "# NOTE: Each example in the data variable has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "\n",
    "import pandas as pd\n",
    "df_no_hash = pd.DataFrame(data_no_hash, columns=[\"sentence\", \"tags\"])\n",
    "df_no_hash[\"class_labels\"] = tag_to_class_mapping_for_data_no_hash\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits using train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df_no_hash, val_df_no_hash = train_test_split(df_no_hash, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df_dataset_train_no_hash = Dataset.from_pandas(train_df_no_hash)\n",
    "df_dataset_val_no_hash = Dataset.from_pandas(val_df_no_hash)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence:   [['Then', 'her', 'arm', '.', '.', '.'], ['#', 'Good', '-', 'bye', ',', 'I', 'showed', 'you', 'a', 'real', 'good', 'time', '#']]\n",
      "Tokens:  ['▁then', '▁her', '▁arm', '▁', '.', '▁', '.', '▁', '.', '</s>']\n",
      "Tokenized Inputs:  {'input_ids': [[98, 67, 903, 4, 5, 4, 5, 4, 5, 1], [4, 899, 123, 4, 26, 735, 4, 8, 4, 13, 1387, 15, 4, 12, 490, 123, 135, 4, 899, 1]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "Examples:  {'sentence': [['Then', 'her', 'arm', '.', '.', '.'], ['#', 'Good', '-', 'bye', ',', 'I', 'showed', 'you', 'a', 'real', 'good', 'time', '#']], 'tags': [['RB', 'PRP$', 'NN', '.', '.', '.'], ['NNP', ':', 'NN', ',', 'PRP', 'VBD', 'PRP', 'DT', 'JJ', 'JJ', 'NN']], 'class_labels': [[25, 17, 11, 21, 21, 21], [38, 14, 11, 22, 13, 28, 13, 12, 7, 7, 11]], '__index_level_0__': [100967, 811154]}\n",
      "Word IDs:  [0, 1, 2, 3, 3, 4, 4, 5, 5, None]\n",
      "Word ID:  0\n",
      "Word ID:  1\n",
      "Word ID:  2\n",
      "Word ID:  3\n",
      "Word ID:  3\n",
      "Inside else\n",
      "Word ID:  4\n",
      "Word ID:  4\n",
      "Inside else\n",
      "Word ID:  5\n",
      "Word ID:  5\n",
      "Inside else\n",
      "Word ID:  None\n",
      "Word IDs:  [0, 0, 1, 2, 2, 3, 4, 4, 5, 5, 6, 7, 8, 8, 9, 10, 11, 12, 12, None]\n",
      "Word ID:  0\n",
      "Word ID:  0\n",
      "Inside else\n",
      "Word ID:  1\n",
      "Word ID:  2\n",
      "Word ID:  2\n",
      "Inside else\n",
      "Word ID:  3\n",
      "Word ID:  4\n",
      "Word ID:  4\n",
      "Inside else\n",
      "Word ID:  5\n",
      "Word ID:  5\n",
      "Inside else\n",
      "Word ID:  6\n",
      "Word ID:  7\n",
      "Word ID:  8\n",
      "Word ID:  8\n",
      "Inside else\n",
      "Word ID:  9\n",
      "Word ID:  10\n",
      "Word ID:  11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m test_df_no_hash \u001b[38;5;241m=\u001b[39m df_dataset_train_no_hash\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1319\u001b[39m,\u001b[38;5;241m1321\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m test_df_tokenized_train_no_hash \u001b[38;5;241m=\u001b[39m \u001b[43mtest_df_no_hash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenize_and_align_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatched\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremove_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_df_no_hash\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumn_names\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest DF Tokenized Train: \u001b[39m\u001b[38;5;124m\"\u001b[39m, test_df_tokenized_train_no_hash[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:593\u001b[0m, in \u001b[0;36mtransmit_tasks.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    592\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 593\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    594\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    595\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset \u001b[38;5;129;01min\u001b[39;00m datasets:\n\u001b[1;32m    596\u001b[0m     \u001b[38;5;66;03m# Remove task templates if a column mapping of the template is no longer valid\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:558\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    555\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    556\u001b[0m }\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 558\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    559\u001b[0m datasets: List[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:3105\u001b[0m, in \u001b[0;36mDataset.map\u001b[0;34m(self, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, load_from_cache_file, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, num_proc, suffix_template, new_fingerprint, desc)\u001b[0m\n\u001b[1;32m   3099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m transformed_dataset \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   3100\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m hf_tqdm(\n\u001b[1;32m   3101\u001b[0m         unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m examples\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3102\u001b[0m         total\u001b[38;5;241m=\u001b[39mpbar_total,\n\u001b[1;32m   3103\u001b[0m         desc\u001b[38;5;241m=\u001b[39mdesc \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMap\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   3104\u001b[0m     ) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[0;32m-> 3105\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m rank, done, content \u001b[38;5;129;01min\u001b[39;00m Dataset\u001b[38;5;241m.\u001b[39m_map_single(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mdataset_kwargs):\n\u001b[1;32m   3106\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m   3107\u001b[0m                 shards_done \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:3482\u001b[0m, in \u001b[0;36mDataset._map_single\u001b[0;34m(shard, function, with_indices, with_rank, input_columns, batched, batch_size, drop_last_batch, remove_columns, keep_in_memory, cache_file_name, writer_batch_size, features, disable_nullable, fn_kwargs, new_fingerprint, rank, offset)\u001b[0m\n\u001b[1;32m   3478\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[1;32m   3479\u001b[0m     \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m*\u001b[39m(\u001b[38;5;28mslice\u001b[39m(i, i \u001b[38;5;241m+\u001b[39m batch_size)\u001b[38;5;241m.\u001b[39mindices(shard\u001b[38;5;241m.\u001b[39mnum_rows)))\n\u001b[1;32m   3480\u001b[0m )  \u001b[38;5;66;03m# Something simpler?\u001b[39;00m\n\u001b[1;32m   3481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3482\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[43mapply_function_on_filtered_inputs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3485\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_same_num_examples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mshard\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlist_indexes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3486\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3487\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3488\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m NumExamplesMismatchError:\n\u001b[1;32m   3489\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetTransformationNotAllowedError(\n\u001b[1;32m   3490\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `.map` in batched mode on a dataset with attached indexes is allowed only if it doesn\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt create or remove existing examples. You can first run `.drop_index() to remove your index and then re-add it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3491\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:3361\u001b[0m, in \u001b[0;36mDataset._map_single.<locals>.apply_function_on_filtered_inputs\u001b[0;34m(pa_inputs, indices, check_same_num_examples, offset)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m with_rank:\n\u001b[1;32m   3360\u001b[0m     additional_args \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (rank,)\n\u001b[0;32m-> 3361\u001b[0m processed_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43madditional_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfn_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3362\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed_inputs, LazyDict):\n\u001b[1;32m   3363\u001b[0m     processed_inputs \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m   3364\u001b[0m         k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m processed_inputs\u001b[38;5;241m.\u001b[39mkeys_to_format\n\u001b[1;32m   3365\u001b[0m     }\n",
      "Cell \u001b[0;32mIn[95], line 14\u001b[0m, in \u001b[0;36mtokenize_and_align_labels\u001b[0;34m(examples)\u001b[0m\n\u001b[1;32m     12\u001b[0m     word_ids \u001b[38;5;241m=\u001b[39m tokenized_inputs\u001b[38;5;241m.\u001b[39mword_ids(i)\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord IDs: \u001b[39m\u001b[38;5;124m\"\u001b[39m, word_ids)\n\u001b[0;32m---> 14\u001b[0m     new_labels\u001b[38;5;241m.\u001b[39mappend(\u001b[43malign_labels_with_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_ids\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     16\u001b[0m tokenized_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m new_labels\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tokenized_inputs\n",
      "Cell \u001b[0;32mIn[85], line 8\u001b[0m, in \u001b[0;36malign_labels_with_tokens\u001b[0;34m(labels, word_ids)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m word_id \u001b[38;5;241m!=\u001b[39m current_word:\n\u001b[1;32m      7\u001b[0m     current_word \u001b[38;5;241m=\u001b[39m word_id\n\u001b[0;32m----> 8\u001b[0m     label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m word_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mlabels\u001b[49m\u001b[43m[\u001b[49m\u001b[43mword_id\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      9\u001b[0m     new_labels\u001b[38;5;241m.\u001b[39mappend(label)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m word_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "test_df_no_hash = df_dataset_train_no_hash.select(range(1319,1321))\n",
    "test_df_tokenized_train_no_hash = test_df_no_hash.map(tokenize_and_align_labels, batched=True, remove_columns=test_df_no_hash.column_names)\n",
    "print(\"Test DF Tokenized Train: \", test_df_tokenized_train_no_hash['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "from transformers import BertTokenizer, PreTrainedTokenizerFast\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('/home/rsaha/projects/babylm/src/tokenizer/hf_wordpiece_tokenizer_from_git/')\n",
    "# tokenizer = PreTrainedTokenizerFast.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PreTrainedTokenizerFast(name_or_path='/home/rsaha/projects/babylm/src/tokenizer/hf_wordpiece_tokenizer_from_git/', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '<PERSON>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32768: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32769: AddedToken(\"<PERSON>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe from the sentence, tags, and class labels.\n",
    "# NOTE: Each example in the data variable has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=[\"sentence\", \"tags\"])\n",
    "df[\"class_labels\"] = tag_to_class_mapping_for_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits using train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df_dataset_train = Dataset.from_pandas(train_df)\n",
    "df_dataset_val = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        print(\"Word ID: \", word_id)\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            print(\"Inside else\")\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    print(\"Example sentence:  \", examples[\"sentence\"])\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, is_split_into_words=True, max_length=50\n",
    "    )\n",
    "    print(\"Tokens: \", tokenized_inputs.tokens())\n",
    "    print(\"Tokenized Inputs: \", tokenized_inputs)\n",
    "    print(\"Examples: \", examples)\n",
    "    all_labels = examples[\"class_labels\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        print(\"Word IDs: \", word_ids)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 2/2 [00:00<00:00, 226.38 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example sentence:   [['Then', 'her', 'arm', '.', '.', '.'], ['#', 'Good', '-', 'bye', ',', 'I', 'showed', 'you', 'a', 'real', 'good', 'time', '#']]\n",
      "Tokens:  ['[CLS]', 'then', 'her', 'arm', '.', '.', '.', '[SEP]']\n",
      "Tokenized Inputs:  {'input_ids': [[101, 2059, 2014, 2849, 1012, 1012, 1012, 102], [101, 1001, 2204, 1011, 9061, 1010, 1045, 3662, 2017, 1037, 2613, 2204, 2051, 1001, 102]], 'token_type_ids': [[0, 0, 0, 0, 0, 0, 0, 0], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]}\n",
      "Examples:  {'sentence': [['Then', 'her', 'arm', '.', '.', '.'], ['#', 'Good', '-', 'bye', ',', 'I', 'showed', 'you', 'a', 'real', 'good', 'time', '#']], 'tags': [['RB', 'PRP$', 'NN', '.', '.', '.'], ['#', 'NNP', ':', 'NN', ',', 'PRP', 'VBD', 'PRP', 'DT', 'JJ', 'JJ', 'NN', '#']], 'class_labels': [[25, 17, 11, 21, 21, 21], [45, 38, 14, 11, 22, 13, 28, 13, 12, 7, 7, 11, 45]], '__index_level_0__': [100967, 811154]}\n",
      "Word IDs:  [None, 0, 1, 2, 3, 4, 5, None]\n",
      "Word ID:  None\n",
      "Word ID:  0\n",
      "Word ID:  1\n",
      "Word ID:  2\n",
      "Word ID:  3\n",
      "Word ID:  4\n",
      "Word ID:  5\n",
      "Word ID:  None\n",
      "Word IDs:  [None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, None]\n",
      "Word ID:  None\n",
      "Word ID:  0\n",
      "Word ID:  1\n",
      "Word ID:  2\n",
      "Word ID:  3\n",
      "Word ID:  4\n",
      "Word ID:  5\n",
      "Word ID:  6\n",
      "Word ID:  7\n",
      "Word ID:  8\n",
      "Word ID:  9\n",
      "Word ID:  10\n",
      "Word ID:  11\n",
      "Word ID:  12\n",
      "Word ID:  None\n",
      "Test DF Tokenized Train:  [[-100, 25, 17, 11, 21, 21, 21, -100], [-100, 45, 38, 14, 11, 22, 13, 28, 13, 12, 7, 7, 11, 45, -100]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_df = df_dataset_train.select(range(1319,1321))\n",
    "test_df_tokenized_train = test_df.map(tokenize_and_align_labels, batched=True, remove_columns=test_df.column_names)\n",
    "print(\"Test DF Tokenized Train: \", test_df_tokenized_train['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4, 899, 1],\n",
       " 'token_type_ids': [0, 0, 0],\n",
       " 'attention_mask': [1, 1, 1],\n",
       " 'labels': [45, 46, -100]}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df_tokenized_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=20):   0%|          | 0/1386254 [00:00<?, ? examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):   1%|          | 10000/1386254 [00:04<05:59, 3828.47 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):   3%|▎         | 45000/1386254 [00:06<02:07, 10547.18 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):   7%|▋         | 100000/1386254 [00:08<00:54, 23777.44 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  11%|█▏        | 157313/1386254 [00:10<00:29, 41087.02 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  15%|█▌        | 211313/1386254 [00:11<00:28, 41517.27 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  21%|██▏       | 297939/1386254 [00:14<00:28, 37610.64 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  27%|██▋       | 374939/1386254 [00:17<00:42, 23706.26 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  31%|███       | 426565/1386254 [00:18<00:23, 40830.92 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  37%|███▋      | 510878/1386254 [00:20<00:23, 37303.26 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  41%|████▏     | 575191/1386254 [00:23<00:41, 19692.74 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  46%|████▌     | 639504/1386254 [00:24<00:26, 28481.72 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  51%|█████     | 709504/1386254 [00:26<00:21, 31396.09 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  57%|█████▋    | 786817/1386254 [00:29<00:20, 29080.86 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  63%|██████▎   | 873443/1386254 [00:31<00:17, 30025.24 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  66%|██████▋   | 918756/1386254 [00:32<00:11, 40554.40 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  74%|███████▍  | 1023069/1386254 [00:35<00:09, 39091.36 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  76%|███████▋  | 1060382/1386254 [00:36<00:08, 37798.70 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  82%|████████▏ | 1136694/1386254 [00:38<00:07, 32173.99 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20):  90%|████████▉ | 1242006/1386254 [00:41<00:04, 35059.46 examples/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map (num_proc=20): 100%|██████████| 1386254/1386254 [00:49<00:00, 27840.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "df_dataset_tokenized_train = df_dataset_train.map(tokenize_and_align_labels, batched=True,\n",
    "                                      remove_columns=df_dataset_train.column_names, num_proc=20)\n",
    "# df_dataset_tokenized_eval = df_dataset_val.map(tokenize_and_align_labels, batched=True,\n",
    "#                                       remove_columns=df_dataset_train.column_names, num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example:  613\n",
      "example:  1320\n",
      "example:  3197\n",
      "example:  5002\n",
      "example:  5318\n",
      "example:  6776\n",
      "example:  7066\n",
      "example:  7599\n",
      "example:  7799\n",
      "example:  7892\n",
      "example:  8361\n",
      "example:  8429\n",
      "example:  9412\n",
      "example:  10666\n",
      "example:  11025\n",
      "example:  11231\n",
      "example:  11313\n",
      "example:  11617\n",
      "example:  12186\n",
      "example:  12415\n",
      "example:  12498\n",
      "example:  13241\n",
      "example:  13394\n",
      "example:  13574\n",
      "example:  13588\n",
      "example:  14141\n",
      "example:  15207\n",
      "example:  16213\n",
      "example:  16364\n",
      "example:  17249\n",
      "example:  18495\n",
      "example:  18945\n",
      "example:  19589\n",
      "example:  19601\n",
      "example:  20517\n",
      "example:  21093\n",
      "example:  21831\n",
      "example:  23790\n",
      "example:  23950\n",
      "example:  25847\n",
      "example:  27215\n",
      "example:  27644\n",
      "example:  27937\n",
      "example:  28030\n",
      "example:  28534\n",
      "example:  29128\n",
      "example:  29994\n",
      "example:  30590\n",
      "example:  30895\n",
      "example:  31036\n",
      "example:  32047\n",
      "example:  32857\n",
      "example:  33436\n",
      "example:  33506\n",
      "example:  34248\n",
      "example:  34570\n",
      "example:  35504\n",
      "example:  35925\n",
      "example:  36385\n",
      "example:  37959\n",
      "example:  37973\n",
      "example:  40828\n",
      "example:  40830\n",
      "example:  41353\n",
      "example:  42365\n",
      "example:  43339\n",
      "example:  44726\n",
      "example:  44811\n",
      "example:  45572\n",
      "example:  46659\n",
      "example:  48747\n",
      "example:  49080\n",
      "example:  49096\n",
      "example:  49349\n",
      "example:  49665\n",
      "example:  50116\n",
      "example:  50397\n",
      "example:  51570\n",
      "example:  55521\n",
      "example:  55561\n",
      "example:  57914\n",
      "example:  58726\n",
      "example:  59313\n",
      "example:  59496\n",
      "example:  59745\n",
      "example:  60093\n",
      "example:  60535\n",
      "example:  60821\n",
      "example:  61147\n",
      "example:  61510\n",
      "example:  61827\n",
      "example:  64176\n",
      "example:  64462\n",
      "example:  64785\n",
      "example:  65913\n",
      "example:  66086\n",
      "example:  66540\n",
      "example:  66932\n",
      "example:  67040\n",
      "example:  67147\n",
      "example:  67393\n",
      "example:  67713\n",
      "example:  67875\n",
      "example:  68980\n",
      "example:  69125\n",
      "example:  69886\n",
      "example:  70256\n",
      "example:  70434\n",
      "example:  71076\n",
      "example:  71248\n",
      "example:  71420\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Figure out which of the labels array in each element of df_dataset_tokenized_train contains 46.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, example \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(df_dataset_tokenized_train):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m46\u001b[39m \u001b[38;5;129;01min\u001b[39;00m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexample: \u001b[39m\u001b[38;5;124m\"\u001b[39m, i)\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/arrow_dataset.py:2389\u001b[0m, in \u001b[0;36mDataset.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2387\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pa_subtable\u001b[38;5;241m.\u001b[39mnum_rows):\n\u001b[1;32m   2388\u001b[0m             pa_subtable_ex \u001b[38;5;241m=\u001b[39m pa_subtable\u001b[38;5;241m.\u001b[39mslice(i, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 2389\u001b[0m             formatted_output \u001b[38;5;241m=\u001b[39m \u001b[43mformat_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2390\u001b[0m \u001b[43m                \u001b[49m\u001b[43mpa_subtable_ex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2391\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2392\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformatter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mformatter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2393\u001b[0m \u001b[43m                \u001b[49m\u001b[43mformat_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_format_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2394\u001b[0m \u001b[43m                \u001b[49m\u001b[43moutput_all_columns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_output_all_columns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2395\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2396\u001b[0m             \u001b[38;5;28;01myield\u001b[39;00m formatted_output\n\u001b[1;32m   2397\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/formatting/formatting.py:629\u001b[0m, in \u001b[0;36mformat_table\u001b[0;34m(table, key, formatter, format_columns, output_all_columns)\u001b[0m\n\u001b[1;32m    627\u001b[0m python_formatter \u001b[38;5;241m=\u001b[39m PythonFormatter(features\u001b[38;5;241m=\u001b[39mformatter\u001b[38;5;241m.\u001b[39mfeatures)\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m format_columns \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mformatter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquery_type\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    631\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m format_columns:\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/formatting/formatting.py:396\u001b[0m, in \u001b[0;36mFormatter.__call__\u001b[0;34m(self, pa_table, query_type)\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable, query_type: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[RowFormat, ColumnFormat, BatchFormat]:\n\u001b[1;32m    395\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 396\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    397\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m query_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumn\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    398\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat_column(pa_table)\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/formatting/formatting.py:436\u001b[0m, in \u001b[0;36mPythonFormatter.format_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    434\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlazy:\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LazyRow(pa_table, \u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m--> 436\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpython_arrow_extractor\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_row\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    437\u001b[0m row \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpython_features_decoder\u001b[38;5;241m.\u001b[39mdecode_row(row)\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m row\n",
      "File \u001b[0;32m~/venvs/cl_dreamscape/lib/python3.10/site-packages/datasets/formatting/formatting.py:144\u001b[0m, in \u001b[0;36mPythonArrowExtractor.extract_row\u001b[0;34m(self, pa_table)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_row\u001b[39m(\u001b[38;5;28mself\u001b[39m, pa_table: pa\u001b[38;5;241m.\u001b[39mTable) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mdict\u001b[39m:\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unnest(\u001b[43mpa_table\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_pydict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Figure out which of the labels array in each element of df_dataset_tokenized_train contains 46.\n",
    "for i, example in enumerate(df_dataset_tokenized_train):\n",
    "    if 46 in example[\"labels\"]:\n",
    "        print(\"example: \", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [4, 899, 1], 'token_type_ids': [0, 0, 0], 'attention_mask': [1, 1, 1], 'labels': [45, 46, -100]}\n",
      "sentence         [#]\n",
      "tags             [#]\n",
      "class_labels    [45]\n",
      "Name: 1584082, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(df_dataset_tokenized_train[613])\n",
    "print(train_df.iloc[613])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=PreTrainedTokenizerFast(name_or_path='/home/rsaha/projects/babylm/src/tokenizer/hf_wordpiece_tokenizer_from_git/', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '<PERSON>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32768: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32769: AddedToken(\"<PERSON>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  13,   14,   28,   11,    1,   36,   13,   42,   13,   14,   10,   13,\n",
      "            1,   36,   38,   22,   22,   12,   11,   21,   22, -100],\n",
      "        [  38,   38,   38,   38,   25,    2,   29,   12,   12,   11,   21,   22,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [  13,   14,   10,    7,   11,   29,   40,   40,   21,   22, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100],\n",
      "        [  38,   38,   38,   25,   28,   13,   29,   21,   22, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "[13, 14, 28, 11, 1, 36, 13, 42, 13, 14, 10, 13, 1, 36, 38, 22, 22, 12, 11, 21, 22, -100]\n",
      "[38, 38, 38, 38, 25, 2, 29, 12, 12, 11, 21, 22, -100]\n",
      "[13, 14, 10, 7, 11, 29, 40, 40, 21, 22, -100]\n",
      "[38, 38, 38, 25, 28, 13, 29, 21, 22, -100]\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([df_dataset_tokenized_train[i] for i in range(4)])\n",
    "print(batch[\"labels\"])\n",
    "for i in range(4):\n",
    "    print(df_dataset_tokenized_train[i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names.keys())}\n",
    "label2id = {v: k for k, v in id2label.items()}  # This is nothing but the label_names dictionary. But keeping it like this for consistency with the Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LS': 0,\n",
       " 'TO': 1,\n",
       " 'VBN': 2,\n",
       " \"''\": 3,\n",
       " 'WP': 4,\n",
       " 'UH': 5,\n",
       " 'VBG': 6,\n",
       " 'JJ': 7,\n",
       " 'VBZ': 8,\n",
       " '--': 9,\n",
       " 'VBP': 10,\n",
       " 'NN': 11,\n",
       " 'DT': 12,\n",
       " 'PRP': 13,\n",
       " ':': 14,\n",
       " 'WP$': 15,\n",
       " 'NNPS': 16,\n",
       " 'PRP$': 17,\n",
       " 'WDT': 18,\n",
       " '(': 19,\n",
       " ')': 20,\n",
       " '.': 21,\n",
       " ',': 22,\n",
       " '``': 23,\n",
       " '$': 24,\n",
       " 'RB': 25,\n",
       " 'RBR': 26,\n",
       " 'RBS': 27,\n",
       " 'VBD': 28,\n",
       " 'IN': 29,\n",
       " 'FW': 30,\n",
       " 'RP': 31,\n",
       " 'JJR': 32,\n",
       " 'JJS': 33,\n",
       " 'PDT': 34,\n",
       " 'MD': 35,\n",
       " 'VB': 36,\n",
       " 'WRB': 37,\n",
       " 'NNP': 38,\n",
       " 'EX': 39,\n",
       " 'NNS': 40,\n",
       " 'SYM': 41,\n",
       " 'CC': 42,\n",
       " 'CD': 43,\n",
       " 'POS': 44,\n",
       " '#': 45}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, BertConfig, BertForTokenClassification\n",
    "bert_config = BertConfig()\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label=id2label, label2id=label2id)\n",
    "# model = AutoModelForTokenClassification.from_config(\n",
    "#     teacher_model.config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    df_dataset_tokenized_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    df_dataset_tokenized_eval, collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW, Adam\n",
    "\n",
    "# optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "import os\n",
    "os.environ[\"PYTORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"TORCH_USE_CUDA_DSA\"] = \"1\"\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "# accelerator = Accelerator(mixed_precision=\"fp16\")\n",
    "# model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "#     model, optimizer, train_dataloader, eval_dataloader\n",
    "# )\n",
    "import torch\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "# lr_scheduler = get_scheduler(\n",
    "#     \"linear\",\n",
    "#     optimizer=optimizer,\n",
    "#     num_warmup_steps=0,\n",
    "#     num_training_steps=num_training_steps,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForTokenClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=46, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Put the device to the GPU.\n",
    "model.to(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/520422 [04:38<?, ?it/s]\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[70], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_dataloader:\n\u001b[0;32m---> 10\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m \u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcuda:5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m     labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda:5\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in tqdm(range(num_train_epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        input_ids = batch['input_ids'].to(\"cuda:5\")\n",
    "        attention_mask = batch['attention_mask'].to(\"cuda:5\")\n",
    "        labels = batch['labels'].to(\"cuda:5\")\n",
    "        # with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        print(\"batch: \", batch)\n",
    "        loss = outputs.loss\n",
    "            \n",
    "        loss.backward()\n",
    "            # # accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        # scaler.scale(loss).backward()\n",
    "        # scaler.step(optimizer)\n",
    "        # # lr_scheduler.step()\n",
    "        # scaler.update()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  11,   12,    6,   17,    7,   11,    1,   12,   40,   40,   29,   12,\n",
       "           11,   22,   22,   12,   11,   22,   22, -100],\n",
       "        [  38,   22,   22,   40,   22,   22,   11,   12,   12,   13,   21,   22,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [   4,   21,   22, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [  25,   21,   22, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [  13,   35,   36,    4,   28,   13,    1,   36,   21,   22, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [  13,   10,   10,   10,   36,   13,   21,   22, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [  12, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100],\n",
       "        [  29,   38,   38,   21,   22, -100, -100, -100, -100, -100, -100, -100,\n",
       "         -100, -100, -100, -100, -100, -100, -100, -100]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['labels']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is for testing and not part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have three lists: data contains two lists of words and tags, tag_to_class_mapping contains the corresponding class labels.\n",
    "labels = tag_to_class_mapping[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2, 3, 3, 4, 4, 5, 6, 7, 7, 7, 8, 9, 9, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Dataset.from_dict(data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_dreamscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rsaha/venvs/cl_dreamscape/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from pathlib import Path\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.data import load as nltk_load"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a huggingface Dataset from the stored sentence and pos tag data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TQDMBytesReader(object):\n",
    "    # For showing the progress bar while reading the stored pickle file.\n",
    "    def __init__(self, fd, **kwargs):\n",
    "        self.fd = fd\n",
    "        from tqdm import tqdm\n",
    "        self.tqdm = tqdm(**kwargs)\n",
    "\n",
    "    def read(self, size=-1):\n",
    "        bytes = self.fd.read(size)\n",
    "        self.tqdm.update(len(bytes))\n",
    "        return bytes\n",
    "\n",
    "    def readline(self):\n",
    "        bytes = self.fd.readline()\n",
    "        self.tqdm.update(len(bytes))\n",
    "        return bytes\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.tqdm.__enter__()\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, *args, **kwargs):\n",
    "        return self.tqdm.__exit__(*args, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_name):\n",
    "    # if os.path.exists(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\"):\n",
    "    if os.path.exists(f\"/home/rsaha/projects/babylm/src/taggers/data/{file_name}.pkl\"):\n",
    "        print(\"Loading data from file ...\")\n",
    "        with open(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\", \"rb\") as f:\n",
    "            total = os.path.getsize(f\"/home/rsaha/projects/babylm/src/taggers/processed_tagger_data/processed_pos_training_data_{file_name}.pkl\")\n",
    "            with TQDMBytesReader(f, total=total) as pbfd:\n",
    "                up = pickle.Unpickler(pbfd)\n",
    "                X_data, y_data = up.load()\n",
    "            return X_data, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "  }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:  ['/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_switchboard.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_open_subtitles.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_childes.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_simple_wiki.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_bnc_spoken.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_local_narr_captions.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_gutenberg.pkl', '/home/rsaha/projects/babylm/src/taggers/data/pos_tagging_dataset_all_sentences_cc_3M_captions_reduced.pkl']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Paths: 100%|██████████| 8/8 [00:00<00:00, 18724.57it/s]\n"
     ]
    }
   ],
   "source": [
    "data_dir = Path(\"/home/rsaha/projects/babylm/src/taggers/data/\")\n",
    "paths = [str(f) for f in data_dir.glob(\"*\") if f.is_file() and not f.name.endswith(\".DS_Store\") and f.suffix in [\".pkl\"]]\n",
    "print(\"Paths: \", paths)\n",
    "\n",
    "file_names = []\n",
    "# Only select the cc_3m and local_narr files and store it in filtered_paths.\n",
    "# filtered_paths = paths #[]\n",
    "# # for path in paths:\n",
    "# #     if \"cc_3M\" in path or \"local_narr\" in path:\n",
    "# #         filtered_paths.append(path)\n",
    "\n",
    "for path in tqdm(paths, desc=\"Paths\"):\n",
    "\n",
    "    file_name = Path(path).name\n",
    "    # Drop the .train extension\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    file_names.append(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['pos_tagging_dataset_all_sentences_switchboard',\n",
       " 'pos_tagging_dataset_all_sentences_open_subtitles',\n",
       " 'pos_tagging_dataset_all_sentences_childes',\n",
       " 'pos_tagging_dataset_all_sentences_simple_wiki',\n",
       " 'pos_tagging_dataset_all_sentences_bnc_spoken',\n",
       " 'pos_tagging_dataset_all_sentences_local_narr_captions',\n",
       " 'pos_tagging_dataset_all_sentences_gutenberg',\n",
       " 'pos_tagging_dataset_all_sentences_cc_3M_captions_reduced']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tagging_dataset_all_sentences_switchboard\n",
      "pos_tagging_dataset_all_sentences_open_subtitles\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:13<00:00,  1.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos_tagging_dataset_all_sentences_childes\n",
      "pos_tagging_dataset_all_sentences_simple_wiki\n",
      "pos_tagging_dataset_all_sentences_bnc_spoken\n",
      "pos_tagging_dataset_all_sentences_local_narr_captions\n",
      "pos_tagging_dataset_all_sentences_gutenberg\n",
      "pos_tagging_dataset_all_sentences_cc_3M_captions_reduced\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the data from each file_name in file_names.\n",
    "all_X_data = []\n",
    "all_y_data = []\n",
    "for file_name in tqdm(file_names):\n",
    "    print(file_name)\n",
    "    if file_name == \"pos_tagging_dataset_all_sentences_open_subtitles\":\n",
    "        data = pickle.load(open(f\"/home/rsaha/projects/babylm/src/taggers/data/{file_name}.pkl\", \"rb\"))\n",
    "        # all_X_data.extend(X_data)\n",
    "        # all_y_data.extend(y_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky'],\n",
       " ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary of numbers where each tag (the second element in the tuple) is assigned a unique number. This will be the class labels.\n",
    "tagdict = nltk_load('help/tagsets/upenn_tagset.pickle')\n",
    "label_names = {t: i for i, t in enumerate(tagdict.keys())}\n",
    "label_names['#'] = len(label_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LS': 0,\n",
       " 'TO': 1,\n",
       " 'VBN': 2,\n",
       " \"''\": 3,\n",
       " 'WP': 4,\n",
       " 'UH': 5,\n",
       " 'VBG': 6,\n",
       " 'JJ': 7,\n",
       " 'VBZ': 8,\n",
       " '--': 9,\n",
       " 'VBP': 10,\n",
       " 'NN': 11,\n",
       " 'DT': 12,\n",
       " 'PRP': 13,\n",
       " ':': 14,\n",
       " 'WP$': 15,\n",
       " 'NNPS': 16,\n",
       " 'PRP$': 17,\n",
       " 'WDT': 18,\n",
       " '(': 19,\n",
       " ')': 20,\n",
       " '.': 21,\n",
       " ',': 22,\n",
       " '``': 23,\n",
       " '$': 24,\n",
       " 'RB': 25,\n",
       " 'RBR': 26,\n",
       " 'RBS': 27,\n",
       " 'VBD': 28,\n",
       " 'IN': 29,\n",
       " 'FW': 30,\n",
       " 'RP': 31,\n",
       " 'JJR': 32,\n",
       " 'JJS': 33,\n",
       " 'PDT': 34,\n",
       " 'MD': 35,\n",
       " 'VB': 36,\n",
       " 'WRB': 37,\n",
       " 'NNP': 38,\n",
       " 'EX': 39,\n",
       " 'NNS': 40,\n",
       " 'SYM': 41,\n",
       " 'CC': 42,\n",
       " 'CD': 43,\n",
       " 'POS': 44,\n",
       " '#': 45}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1734740/1734740 [00:05<00:00, 308749.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Data has many tuples and each tuple has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "# Create a third separate list of lists where each list contains the number from the label_names dictionary based on the key tag.\n",
    "# This will be the class labels.\n",
    "tag_to_class_mapping = []\n",
    "for i in tqdm(range(len(data))):\n",
    "    tag_to_class_mapping.append([label_names[tag] for tag in data[i][1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def align_labels_with_tokens(labels, word_ids):\n",
    "    new_labels = []\n",
    "    current_word = None\n",
    "    for word_id in word_ids:\n",
    "        if word_id != current_word:\n",
    "            current_word = word_id\n",
    "            label = -100 if word_id is None else labels[word_id]\n",
    "            new_labels.append(label)\n",
    "        elif word_id is None:\n",
    "            new_labels.append(-100)\n",
    "        else:\n",
    "            label = labels[word_id]\n",
    "            if label % 2 == 1:\n",
    "                label += 1\n",
    "            new_labels.append(label)\n",
    "    return new_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer.\n",
    "from transformers import BertTokenizer, PreTrainedTokenizerFast\n",
    "tokenizer = PreTrainedTokenizerFast.from_pretrained('/home/rsaha/projects/babylm/src/tokenizer/hf_wordpiece_tokenizer_from_git/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = tag_to_class_mapping[0]\n",
    "inputs = tokenizer(data[0][0], is_split_into_words=True)\n",
    "word_ids = inputs.word_ids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[13, 10, 29, 13, 10, 25, 1, 12, 11]\n",
      "['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN']\n",
      "[0, 0, 1, 2, 3, 3, 4, 5, 6, 7, 8, None]\n",
      "[13, 14, 10, 29, 13, 14, 10, 25, 1, 12, 11, -100]\n"
     ]
    }
   ],
   "source": [
    "print(labels)\n",
    "print(data[0][1])\n",
    "print(word_ids)\n",
    "print(align_labels_with_tokens(labels, word_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a dataframe from the sentence, tags, and class labels.\n",
    "# NOTE: Each example in the data variable has two lists. The first list is the list of words and the second list is the list of tags.\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(data, columns=[\"sentence\", \"tags\"])\n",
    "df[\"class_labels\"] = tag_to_class_mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create train and validation splits using train_test_split.\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "df_dataset_train = Dataset.from_pandas(train_df)\n",
    "df_dataset_val = Dataset.from_pandas(val_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'tags', 'class_labels', '__index_level_0__'],\n",
       "    num_rows: 1387792\n",
       "})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"sentence\"], truncation=True, is_split_into_words=True\n",
    "    )\n",
    "    all_labels = examples[\"class_labels\"]\n",
    "    new_labels = []\n",
    "    for i, labels in enumerate(all_labels):\n",
    "        word_ids = tokenized_inputs.word_ids(i)\n",
    "        new_labels.append(align_labels_with_tokens(labels, word_ids))\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = new_labels\n",
    "    return tokenized_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map (num_proc=20): 100%|██████████| 1387792/1387792 [00:51<00:00, 27206.27 examples/s]\n",
      "Map (num_proc=20): 100%|██████████| 346948/346948 [00:15<00:00, 22963.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "df_dataset_tokenized_train = df_dataset_train.map(tokenize_and_align_labels, batched=True,\n",
    "                                      remove_columns=df_dataset_train.column_names, num_proc=20)\n",
    "df_dataset_tokenized_eval = df_dataset_val.map(tokenize_and_align_labels, batched=True,\n",
    "                                      remove_columns=df_dataset_train.column_names, num_proc=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [4,\n",
       "  13,\n",
       "  107,\n",
       "  183,\n",
       "  18,\n",
       "  176,\n",
       "  15,\n",
       "  16,\n",
       "  4,\n",
       "  13,\n",
       "  199,\n",
       "  15,\n",
       "  18,\n",
       "  176,\n",
       "  2083,\n",
       "  4,\n",
       "  8,\n",
       "  71,\n",
       "  94,\n",
       "  4,\n",
       "  17,\n",
       "  1],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1],\n",
       " 'labels': [13,\n",
       "  14,\n",
       "  28,\n",
       "  11,\n",
       "  1,\n",
       "  36,\n",
       "  13,\n",
       "  42,\n",
       "  13,\n",
       "  14,\n",
       "  10,\n",
       "  13,\n",
       "  1,\n",
       "  36,\n",
       "  38,\n",
       "  22,\n",
       "  22,\n",
       "  12,\n",
       "  11,\n",
       "  21,\n",
       "  22,\n",
       "  -100]}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dataset_tokenized_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataCollatorForTokenClassification(tokenizer=PreTrainedTokenizerFast(name_or_path='/home/rsaha/projects/babylm/src/tokenizer/hf_wordpiece_tokenizer_from_git/', vocab_size=32768, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'pad_token': '<pad>', 'additional_special_tokens': ['<image>', '<PERSON>']}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t3: AddedToken(\"<mask>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32768: AddedToken(\"<image>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32769: AddedToken(\"<PERSON>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}, padding=True, max_length=None, pad_to_multiple_of=None, label_pad_token_id=-100, return_tensors='pt')"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  13,   14,   28,   11,    1,   36,   13,   42,   13,   14,   10,   13,\n",
      "            1,   36,   38,   22,   22,   12,   11,   21,   22, -100],\n",
      "        [  38,   38,   38,   38,   25,    2,   29,   12,   12,   11,   21,   22,\n",
      "         -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]])\n",
      "[13, 14, 28, 11, 1, 36, 13, 42, 13, 14, 10, 13, 1, 36, 38, 22, 22, 12, 11, 21, 22, -100]\n",
      "[38, 38, 38, 38, 25, 2, 29, 12, 12, 11, 21, 22, -100]\n"
     ]
    }
   ],
   "source": [
    "batch = data_collator([df_dataset_tokenized_train[i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "for i in range(2):\n",
    "    print(df_dataset_tokenized_train[i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {i: label for i, label in enumerate(label_names.keys())}\n",
    "label2id = {v: k for k, v in id2label.items()}  # This is nothing but the label_names dictionary. But keeping it like this for consistency with the Kaggle notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'LS': 0,\n",
       " 'TO': 1,\n",
       " 'VBN': 2,\n",
       " \"''\": 3,\n",
       " 'WP': 4,\n",
       " 'UH': 5,\n",
       " 'VBG': 6,\n",
       " 'JJ': 7,\n",
       " 'VBZ': 8,\n",
       " '--': 9,\n",
       " 'VBP': 10,\n",
       " 'NN': 11,\n",
       " 'DT': 12,\n",
       " 'PRP': 13,\n",
       " ':': 14,\n",
       " 'WP$': 15,\n",
       " 'NNPS': 16,\n",
       " 'PRP$': 17,\n",
       " 'WDT': 18,\n",
       " '(': 19,\n",
       " ')': 20,\n",
       " '.': 21,\n",
       " ',': 22,\n",
       " '``': 23,\n",
       " '$': 24,\n",
       " 'RB': 25,\n",
       " 'RBR': 26,\n",
       " 'RBS': 27,\n",
       " 'VBD': 28,\n",
       " 'IN': 29,\n",
       " 'FW': 30,\n",
       " 'RP': 31,\n",
       " 'JJR': 32,\n",
       " 'JJS': 33,\n",
       " 'PDT': 34,\n",
       " 'MD': 35,\n",
       " 'VB': 36,\n",
       " 'WRB': 37,\n",
       " 'NNP': 38,\n",
       " 'EX': 39,\n",
       " 'NNS': 40,\n",
       " 'SYM': 41,\n",
       " 'CC': 42,\n",
       " 'CD': 43,\n",
       " 'POS': 44,\n",
       " '#': 45}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, BertConfig, BertForTokenClassification\n",
    "bert_config = BertConfig()\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "teacher_model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, id2label=id2label, label2id=label2id)\n",
    "model = AutoModelForTokenClassification.from_config(\n",
    "    teacher_model.config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"seqeval\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    logits, labels = eval_preds\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    all_metrics = metric.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": all_metrics[\"overall_precision\"],\n",
    "        \"recall\": all_metrics[\"overall_recall\"],\n",
    "        \"f1\": all_metrics[\"overall_f1\"],\n",
    "        \"accuracy\": all_metrics[\"overall_accuracy\"],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataloader = DataLoader(\n",
    "    df_dataset_tokenized_train,\n",
    "    shuffle=True,\n",
    "    collate_fn=data_collator,\n",
    "    batch_size=8,\n",
    ")\n",
    "eval_dataloader = DataLoader(\n",
    "    df_dataset_tokenized_eval, collate_fn=data_collator, batch_size=8\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from accelerate import Accelerator\n",
    "\n",
    "accelerator = Accelerator()\n",
    "model, optimizer, train_dataloader, eval_dataloader = accelerator.prepare(\n",
    "    model, optimizer, train_dataloader, eval_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_scheduler\n",
    "\n",
    "num_train_epochs = 3\n",
    "num_update_steps_per_epoch = len(train_dataloader)\n",
    "num_training_steps = num_train_epochs * num_update_steps_per_epoch\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    \"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(predictions, labels):\n",
    "    predictions = predictions.detach().cpu().clone().numpy()\n",
    "    labels = labels.detach().cpu().clone().numpy()\n",
    "\n",
    "    # Remove ignored index (special tokens) and convert to labels\n",
    "    true_labels = [[label_names[l] for l in label if l != -100] for label in labels]\n",
    "    true_predictions = [\n",
    "        [label_names[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return true_labels, true_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "\n",
    "for epoch in tqdm(range(num_train_epochs)):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for batch in train_dataloader:\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        accelerator.backward(loss)\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    for batch in eval_dataloader:\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**batch)\n",
    "\n",
    "        predictions = outputs.logits.argmax(dim=-1)\n",
    "        labels = batch[\"labels\"]\n",
    "\n",
    "        # Necessary to pad predictions and labels for being gathered\n",
    "        predictions = accelerator.pad_across_processes(predictions, dim=1, pad_index=-100)\n",
    "        labels = accelerator.pad_across_processes(labels, dim=1, pad_index=-100)\n",
    "\n",
    "        predictions_gathered = accelerator.gather(predictions)\n",
    "        labels_gathered = accelerator.gather(labels)\n",
    "\n",
    "        true_predictions, true_labels = postprocess(predictions_gathered, labels_gathered)\n",
    "        metric.add_batch(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "    results = metric.compute()\n",
    "    print(\n",
    "        f\"epoch {epoch}:\",\n",
    "        {\n",
    "            key: results[f\"overall_{key}\"]\n",
    "            for key in [\"precision\", \"recall\", \"f1\", \"accuracy\"]\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code below is for testing and not part of the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now I have three lists: data contains two lists of words and tags, tag_to_class_mapping contains the corresponding class labels.\n",
    "labels = tag_to_class_mapping[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 2, 3, 3, 4, 4, 5, 6, 7, 7, 7, 8, 9, 9, None]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.word_ids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = data_collator([tokenized_datasets[\"train\"][i] for i in range(2)])\n",
    "print(batch[\"labels\"])\n",
    "for i in range(2):\n",
    "    print(tokenized_datasets[\"train\"][i][\"labels\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = datasets.Dataset.from_dict(data_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cl_dreamscape",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

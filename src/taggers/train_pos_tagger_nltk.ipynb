{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "  }\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "  X, y = [], []\n",
    "  for sentence, tags in tagged_sentences:\n",
    "    sent_word_features, sent_tags = [],[]\n",
    "    for index in range(len(sentence)):\n",
    "        sent_word_features.append(extract_features(sentence, index)),\n",
    "        sent_tags.append(tags[index])\n",
    "    X.append(sent_word_features)\n",
    "    y.append(sent_tags)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
    "\n",
    "#No need to install nltk in google colab since it is preloaded in the environments.\n",
    "#!pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('treebank')\n",
    "\n",
    "# #Ensure that the treebank corpus is downloaded\n",
    "\n",
    "# #Load the treebank corpus class\n",
    "# from nltk.corpus import treebank\n",
    "\n",
    "# #Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences) \n",
    "# #and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with \n",
    "# #a list of words and a list of their respective PoS tags (in the same order).\n",
    "# penn_treebank = []\n",
    "# for fileid in treebank.fileids():\n",
    "#   tokens = []\n",
    "#   tags = []\n",
    "#   for word, tag in treebank.tagged_words(fileid):\n",
    "#     tokens.append(word)\n",
    "#     tags.append(tag)\n",
    "#   penn_treebank.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths:  ['../../data/train_50M_multimodal_clean/open_subtitles.train', '../../data/train_50M_multimodal_clean/childes.train', '../../data/train_50M_multimodal_clean/cc_3M_captions_reduced.train', '../../data/train_50M_multimodal_clean/bnc_spoken.train', '../../data/train_50M_multimodal_clean/gutenberg.train', '../../data/train_50M_multimodal_clean/simple_wiki.train', '../../data/train_50M_multimodal_clean/switchboard.train', '../../data/train_50M_multimodal_clean/local_narr_captions.train']\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"../../data/train_50M_multimodal_clean/\")\n",
    "paths = [str(f) for f in data_dir.glob(\"*\") if f.is_file() and not f.name.endswith(\".DS_Store\") and f.suffix in [\".train\"]]\n",
    "print(\"Paths: \", paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from pathlib import Path\n",
    "\n",
    "def process_text_file(file_path):\n",
    "    words = []\n",
    "    tags = []\n",
    "    f = open(file_path, 'rb')\n",
    "    n = sum(1 for _ in f)  # count the number of lines in the file\n",
    "    print(\"Total lines in file: \", n)\n",
    "    f.close()\n",
    "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|\\b\\w+(?:-\\w+)*\\b|\\d+(?:\\.\\d+)?|\\S\"  # Only consider the words.\n",
    "    k = 0\n",
    "    sentences_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            sentence = line.strip()\n",
    "\n",
    "            # Split the sentence using the refined regex pattern\n",
    "            tokens = re.findall(pattern, sentence)\n",
    "            tagged_sentence = pos_tag(tokens)\n",
    "            # print(tagged_sentence)\n",
    "            \n",
    "            for word, tag in tagged_sentence:\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            k += 1\n",
    "            sentences_list.append((words, tags))\n",
    "            \n",
    "            print(\"Completed line {0} out of {1}\".format(k, n), end=\"\\r\")\n",
    "    return sentences_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path:  ../../data/train_50M_multimodal_clean/open_subtitles.train\n",
      "Total lines in file:  1734740\n",
      "Global list:  [(['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN']), (['I', 'cry', 'as', 'I', 'look', 'up', 'to', 'the', 'sky', 'Where', 'our', 'promise', 'shines', 'brightly', 'Try', '!', 'Comforting', 'words', 'are', 'like', 'breezes', 'that', \"don't\", 'reach', 'my', 'heart', 'My', 'misgivings', 'grow', 'But', 'I', 'make', 'my', 'own', 'decisions', 'so', 'I', 'will', 'probably', 'be', 'all', 'right', 'on', 'my', 'own', 'Even', 'if', 'I', 'stumble', ',', 'even', 'if', 'I', 'get', 'lost', ',', \"there's\", 'always', 'tomorrow', 'd', 'You', 'can', 'do', 'what', 'you', 'wanna', 'do', 'd', 'd', 'In', 'living', 'colord'], ['PRP', 'VBP', 'IN', 'PRP', 'VBP', 'RB', 'TO', 'DT', 'NN', 'WRB', 'PRP\n",
      ", 'NN', 'NNS', 'RB', 'VB', '.', 'VBG', 'NNS', 'VBP', 'IN', 'NNS', 'WDT', 'VBP', 'VB', 'PRP\n",
      ", 'NN', 'PRP\n",
      ", 'NNS', 'VB', 'CC', 'PRP', 'VBP', 'PRP\n",
      ", 'JJ', 'NNS', 'RB', 'PRP', 'MD', 'RB', 'VB', 'RB', 'RB', 'IN', 'PRP\n",
      ", 'JJ', 'RB', 'IN', 'PRP', 'VBP', ',', 'RB', 'IN', 'PRP', 'VBP', 'VBN', ',', 'NN', 'RB', 'NN', 'NN', 'PRP', 'MD', 'VB', 'WP', 'PRP', 'VBP', 'VB', 'JJ', 'NN', 'IN', 'NN', 'NN'])]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "global_list = []\n",
    "for file_path in paths:\n",
    "    # file_path = paths[i]\n",
    "    print(\"File path: \", file_path)\n",
    "    result = process_text_file(file_path)\n",
    "    global_list.extend(result)\n",
    "    print(\"Global list: \",global_list)\n",
    "\n",
    "# global_list_set = set(global_list)  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['I',\n",
       "   'cry',\n",
       "   'as',\n",
       "   'I',\n",
       "   'look',\n",
       "   'up',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sky',\n",
       "   'Where',\n",
       "   'our',\n",
       "   'promise',\n",
       "   'shines',\n",
       "   'brightly',\n",
       "   'Try',\n",
       "   '!',\n",
       "   'Comforting',\n",
       "   'words',\n",
       "   'are',\n",
       "   'like',\n",
       "   'breezes',\n",
       "   'that',\n",
       "   \"don't\",\n",
       "   'reach',\n",
       "   'my',\n",
       "   'heart',\n",
       "   'My',\n",
       "   'misgivings',\n",
       "   'grow',\n",
       "   'But',\n",
       "   'I',\n",
       "   'make',\n",
       "   'my',\n",
       "   'own',\n",
       "   'decisions',\n",
       "   'so',\n",
       "   'I',\n",
       "   'will',\n",
       "   'probably',\n",
       "   'be',\n",
       "   'all',\n",
       "   'right',\n",
       "   'on',\n",
       "   'my',\n",
       "   'own',\n",
       "   'Even',\n",
       "   'if',\n",
       "   'I',\n",
       "   'stumble',\n",
       "   ',',\n",
       "   'even',\n",
       "   'if',\n",
       "   'I',\n",
       "   'get',\n",
       "   'lost',\n",
       "   ',',\n",
       "   \"there's\",\n",
       "   'always',\n",
       "   'tomorrow',\n",
       "   'd',\n",
       "   'You',\n",
       "   'can',\n",
       "   'do',\n",
       "   'what',\n",
       "   'you',\n",
       "   'wanna',\n",
       "   'do',\n",
       "   'd',\n",
       "   'd',\n",
       "   'In',\n",
       "   'living',\n",
       "   'colord'],\n",
       "  ['PRP',\n",
       "   'VBP',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'RB',\n",
       "   'TO',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'WRB',\n",
       "   'PRP$',\n",
       "   'NN',\n",
       "   'NNS',\n",
       "   'RB',\n",
       "   'VB',\n",
       "   '.',\n",
       "   'VBG',\n",
       "   'NNS',\n",
       "   'VBP',\n",
       "   'IN',\n",
       "   'NNS',\n",
       "   'WDT',\n",
       "   'VBP',\n",
       "   'VB',\n",
       "   'PRP$',\n",
       "   'NN',\n",
       "   'PRP$',\n",
       "   'NNS',\n",
       "   'VB',\n",
       "   'CC',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'PRP$',\n",
       "   'JJ',\n",
       "   'NNS',\n",
       "   'RB',\n",
       "   'PRP',\n",
       "   'MD',\n",
       "   'RB',\n",
       "   'VB',\n",
       "   'RB',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP$',\n",
       "   'JJ',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   ',',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'VBN',\n",
       "   ',',\n",
       "   'NN',\n",
       "   'RB',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'PRP',\n",
       "   'MD',\n",
       "   'VB',\n",
       "   'WP',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'VB',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'NN',\n",
       "   'NN']),\n",
       " (['I',\n",
       "   'cry',\n",
       "   'as',\n",
       "   'I',\n",
       "   'look',\n",
       "   'up',\n",
       "   'to',\n",
       "   'the',\n",
       "   'sky',\n",
       "   'Where',\n",
       "   'our',\n",
       "   'promise',\n",
       "   'shines',\n",
       "   'brightly',\n",
       "   'Try',\n",
       "   '!',\n",
       "   'Comforting',\n",
       "   'words',\n",
       "   'are',\n",
       "   'like',\n",
       "   'breezes',\n",
       "   'that',\n",
       "   \"don't\",\n",
       "   'reach',\n",
       "   'my',\n",
       "   'heart',\n",
       "   'My',\n",
       "   'misgivings',\n",
       "   'grow',\n",
       "   'But',\n",
       "   'I',\n",
       "   'make',\n",
       "   'my',\n",
       "   'own',\n",
       "   'decisions',\n",
       "   'so',\n",
       "   'I',\n",
       "   'will',\n",
       "   'probably',\n",
       "   'be',\n",
       "   'all',\n",
       "   'right',\n",
       "   'on',\n",
       "   'my',\n",
       "   'own',\n",
       "   'Even',\n",
       "   'if',\n",
       "   'I',\n",
       "   'stumble',\n",
       "   ',',\n",
       "   'even',\n",
       "   'if',\n",
       "   'I',\n",
       "   'get',\n",
       "   'lost',\n",
       "   ',',\n",
       "   \"there's\",\n",
       "   'always',\n",
       "   'tomorrow',\n",
       "   'd',\n",
       "   'You',\n",
       "   'can',\n",
       "   'do',\n",
       "   'what',\n",
       "   'you',\n",
       "   'wanna',\n",
       "   'do',\n",
       "   'd',\n",
       "   'd',\n",
       "   'In',\n",
       "   'living',\n",
       "   'colord'],\n",
       "  ['PRP',\n",
       "   'VBP',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'RB',\n",
       "   'TO',\n",
       "   'DT',\n",
       "   'NN',\n",
       "   'WRB',\n",
       "   'PRP$',\n",
       "   'NN',\n",
       "   'NNS',\n",
       "   'RB',\n",
       "   'VB',\n",
       "   '.',\n",
       "   'VBG',\n",
       "   'NNS',\n",
       "   'VBP',\n",
       "   'IN',\n",
       "   'NNS',\n",
       "   'WDT',\n",
       "   'VBP',\n",
       "   'VB',\n",
       "   'PRP$',\n",
       "   'NN',\n",
       "   'PRP$',\n",
       "   'NNS',\n",
       "   'VB',\n",
       "   'CC',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'PRP$',\n",
       "   'JJ',\n",
       "   'NNS',\n",
       "   'RB',\n",
       "   'PRP',\n",
       "   'MD',\n",
       "   'RB',\n",
       "   'VB',\n",
       "   'RB',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP$',\n",
       "   'JJ',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   ',',\n",
       "   'RB',\n",
       "   'IN',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'VBN',\n",
       "   ',',\n",
       "   'NN',\n",
       "   'RB',\n",
       "   'NN',\n",
       "   'NN',\n",
       "   'PRP',\n",
       "   'MD',\n",
       "   'VB',\n",
       "   'WP',\n",
       "   'PRP',\n",
       "   'VBP',\n",
       "   'VB',\n",
       "   'JJ',\n",
       "   'NN',\n",
       "   'IN',\n",
       "   'NN',\n",
       "   'NN'])]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_list[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(global_list, columns=[\"Word\", \"Tag\"])\n",
    "df.to_csv(\"pos_tagging_dataset_with_duplicates.csv\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"pos_tagging_dataset_with_duplicates_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cry'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ast\n",
    "ast.literal_eval(test['Word'][0])[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a tagger using the saved training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "  }\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "  X, y = [], []\n",
    "  for sentence, tags in tagged_sentences:\n",
    "    sent_word_features, sent_tags = [],[]\n",
    "    for index in range(len(sentence)):\n",
    "        sent_word_features.append(extract_features(sentence, index)),\n",
    "        sent_tags.append(tags[index])\n",
    "    X.append(sent_word_features)\n",
    "    y.append(sent_tags)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset.\n",
    "with_duplicates = False\n",
    "if with_duplicates:\n",
    "    df = pd.read_csv(\"pos_tagging_dataset_with_duplicates.csv\", compression='gzip')\n",
    "else:\n",
    "    df = pd.read_csv(\"pos_tagging_dataset_no_duplicates.csv\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\n",
      "NNP     206205\n",
      "NN      121238\n",
      "JJ       50827\n",
      "NNS      37642\n",
      "VB       23447\n",
      "VBP      16526\n",
      "VBN      15223\n",
      "RB       13977\n",
      "VBZ      13889\n",
      "VBD      13429\n",
      "CD       11658\n",
      "VBG      10499\n",
      "IN        5359\n",
      "SYM       4021\n",
      "NNPS      3278\n",
      "FW        2490\n",
      "JJR       1743\n",
      "CC        1562\n",
      "RP        1146\n",
      "JJS       1010\n",
      "PRP        878\n",
      "PDT        771\n",
      "MD         740\n",
      "RBR        662\n",
      "UH         639\n",
      "DT         613\n",
      "EX         372\n",
      "$          357\n",
      "WDT        338\n",
      "WP         314\n",
      "''         306\n",
      "WRB        270\n",
      "PRP$       248\n",
      "RBS        169\n",
      "LS         114\n",
      "POS         52\n",
      "TO          14\n",
      "``           3\n",
      ":            3\n",
      ".            3\n",
      "(            2\n",
      ")            2\n",
      "WP$          1\n",
      "#            1\n",
      ",            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# testing, will delete this cell.\n",
    "print(df[\"Tag\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the $ tags because they do NOT exist in the PENN Treebank dataset.\n",
    "df = df[~df[\"Tag\"].isin([\"$\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From the dataset, remove all the elements where the TAG is a punctuation.\n",
    "searchfor = ['.', ',', '!', '?', ':', ';', '(', ')', '[', ']', '{', '}', '<', '>', '\"', \"'\", \"``\", \"''\", \"#\"]\n",
    "#  '#', '$', '&', '*', '+', '-', '/', '<', '=', '>', '@', '^', '_', '`', '|', '~',\n",
    "df[\"TrueFalse\"] = df['Tag'].apply(lambda x: 1 if any(i in x for i in searchfor) else 0)\n",
    "# df_no_puncts = df[~df[\"Tag\"].str.contains('|'.join(searchFor))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all the rows where the TrueFalse column is 1.\n",
    "df_no_puncts = df[df[\"TrueFalse\"] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tag\n",
      "NNP     206205\n",
      "NN      121238\n",
      "JJ       50827\n",
      "NNS      37642\n",
      "VB       23447\n",
      "VBP      16526\n",
      "VBN      15223\n",
      "RB       13977\n",
      "VBZ      13889\n",
      "VBD      13429\n",
      "CD       11658\n",
      "VBG      10499\n",
      "IN        5359\n",
      "SYM       4021\n",
      "NNPS      3278\n",
      "FW        2490\n",
      "JJR       1743\n",
      "CC        1562\n",
      "RP        1146\n",
      "JJS       1010\n",
      "PRP        878\n",
      "PDT        771\n",
      "MD         740\n",
      "RBR        662\n",
      "UH         639\n",
      "DT         613\n",
      "EX         372\n",
      "WDT        338\n",
      "WP         314\n",
      "WRB        270\n",
      "PRP$       248\n",
      "RBS        169\n",
      "LS         114\n",
      "POS         52\n",
      "TO          14\n",
      "WP$          1\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_no_puncts[\"Tag\"].value_counts())\n",
    "len(df_no_puncts[\"Tag\"].value_counts())  # The count must be 36 according to this link: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Tag</th>\n",
       "      <th>TrueFalse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kar'sene</td>\n",
       "      <td>NNP</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>subtracted</td>\n",
       "      <td>VBD</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>foolish</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>llgotallythe</td>\n",
       "      <td>NN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>comforted</td>\n",
       "      <td>JJ</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Word  Tag  TrueFalse\n",
       "0      Kar'sene  NNP          0\n",
       "1    subtracted  VBD          0\n",
       "2       foolish   NN          0\n",
       "3  llgotallythe   NN          0\n",
       "4     comforted   JJ          0"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_no_puncts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[62], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m training \u001b[38;5;241m=\u001b[39m df_no_puncts[:train_size]\n\u001b[1;32m      4\u001b[0m testing \u001b[38;5;241m=\u001b[39m df_no_puncts[train_size:]\n\u001b[0;32m----> 5\u001b[0m X_train, y_train \u001b[38;5;241m=\u001b[39m \u001b[43mtransform_to_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m X_test, y_test \u001b[38;5;241m=\u001b[39m transform_to_dataset(testing)\n",
      "Cell \u001b[0;32mIn[2], line 27\u001b[0m, in \u001b[0;36mtransform_to_dataset\u001b[0;34m(tagged_sentences)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtransform_to_dataset\u001b[39m(tagged_sentences):\n\u001b[1;32m     26\u001b[0m   X, y \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m---> 27\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m sentence, tags \u001b[38;5;129;01min\u001b[39;00m tagged_sentences:\n\u001b[1;32m     28\u001b[0m     sent_word_features, sent_tags \u001b[38;5;241m=\u001b[39m [],[]\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m index \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(sentence)):\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# Create train test split.\n",
    "train_size = int(0.8*len(df_no_puncts))\n",
    "training = df_no_puncts[:train_size]\n",
    "testing = df_no_puncts[train_size:]\n",
    "X_train, y_train = transform_to_dataset(training)\n",
    "X_test, y_test = transform_to_dataset(testing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('school', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['I','am','going','to','school', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined regex pattern\n",
    "text = \"Here's an example sentence: with numbers 123 and punctuations, hyphens - and more! high-speed and it's 3.14 and example@example.com\"\n",
    "\n",
    "# Refined regex pattern\n",
    "pattern = r\"[A-Za-z0-9]+(?:'[A-Za-z]+)?|[A-Za-z]+(?:-[A-Za-z]+)*|[0-9]+(?:\\.[0-9]+)?|[^\\w\\s]\"\n",
    "\n",
    "# Split the sentence using the refined regex pattern\n",
    "tokens = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [extract_features(tokens, i) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': '.',\n",
       " 'is_first': False,\n",
       " 'is_last': False,\n",
       " 'is_capitalized': True,\n",
       " 'is_all_caps': True,\n",
       " 'is_all_lower': True,\n",
       " 'is_alphanumeric': 0,\n",
       " 'prefix-1': '.',\n",
       " 'prefix-2': '.',\n",
       " 'prefix-3': '.',\n",
       " 'suffix-1': '.',\n",
       " 'suffix-2': '.',\n",
       " 'suffix-3': '.',\n",
       " 'prev_word': 'example',\n",
       " 'next_word': '',\n",
       " 'has_hyphen': False,\n",
       " 'is_numeric': False,\n",
       " 'capitals_inside': False,\n",
       " 'punctuation': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's\",\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " ':',\n",
       " 'with',\n",
       " 'numbers',\n",
       " '123',\n",
       " 'and',\n",
       " 'punctuations',\n",
       " ',',\n",
       " 'hyphens',\n",
       " '-',\n",
       " 'and',\n",
       " 'more',\n",
       " '!',\n",
       " 'high',\n",
       " '-',\n",
       " 'speed',\n",
       " 'and',\n",
       " \"it's\",\n",
       " '3',\n",
       " '.',\n",
       " '14',\n",
       " 'and',\n",
       " 'example',\n",
       " '@',\n",
       " 'example',\n",
       " '.',\n",
       " 'com']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

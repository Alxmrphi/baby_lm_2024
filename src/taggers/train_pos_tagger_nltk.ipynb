{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Regex module for checking alphanumeric values.\n",
    "import re\n",
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:],\n",
    "      # 'punctuation': sentence[index] in ',.!?;:(){}[]',\n",
    "  }\n",
    "\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "  X, y = [], []\n",
    "  for sentence, tags in tagged_sentences:\n",
    "    sent_word_features, sent_tags = [],[]\n",
    "    for index in range(len(sentence)):\n",
    "        sent_word_features.append(extract_features(sentence, index)),\n",
    "        sent_tags.append(tags[index])\n",
    "    X.append(sent_word_features)\n",
    "    y.append(sent_tags)\n",
    "  return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to /home/rsaha/nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#This cell loads the Penn Treebank corpus from nltk into a list variable named penn_treebank.\n",
    "\n",
    "#No need to install nltk in google colab since it is preloaded in the environments.\n",
    "#!pip install nltk\n",
    "import nltk\n",
    "nltk.download('treebank')\n",
    "\n",
    "#Ensure that the treebank corpus is downloaded\n",
    "\n",
    "#Load the treebank corpus class\n",
    "from nltk.corpus import treebank\n",
    "\n",
    "#Now we iterate over all samples from the corpus (the fileids - that are equivalent to sentences) \n",
    "#and retrieve the word and the pre-labeled PoS tag. This will be added as a list of tuples with \n",
    "#a list of words and a list of their respective PoS tags (in the same order).\n",
    "penn_treebank = []\n",
    "for fileid in treebank.fileids():\n",
    "  tokens = []\n",
    "  tags = []\n",
    "  for word, tag in treebank.tagged_words(fileid):\n",
    "    tokens.append(word)\n",
    "    tags.append(tag)\n",
    "  penn_treebank.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Pierre',\n",
       "  'Vinken',\n",
       "  ',',\n",
       "  '61',\n",
       "  'years',\n",
       "  'old',\n",
       "  ',',\n",
       "  'will',\n",
       "  'join',\n",
       "  'the',\n",
       "  'board',\n",
       "  'as',\n",
       "  'a',\n",
       "  'nonexecutive',\n",
       "  'director',\n",
       "  'Nov.',\n",
       "  '29',\n",
       "  '.',\n",
       "  'Mr.',\n",
       "  'Vinken',\n",
       "  'is',\n",
       "  'chairman',\n",
       "  'of',\n",
       "  'Elsevier',\n",
       "  'N.V.',\n",
       "  ',',\n",
       "  'the',\n",
       "  'Dutch',\n",
       "  'publishing',\n",
       "  'group',\n",
       "  '.'],\n",
       " ['NNP',\n",
       "  'NNP',\n",
       "  ',',\n",
       "  'CD',\n",
       "  'NNS',\n",
       "  'JJ',\n",
       "  ',',\n",
       "  'MD',\n",
       "  'VB',\n",
       "  'DT',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'DT',\n",
       "  'JJ',\n",
       "  'NN',\n",
       "  'NNP',\n",
       "  'CD',\n",
       "  '.',\n",
       "  'NNP',\n",
       "  'NNP',\n",
       "  'VBZ',\n",
       "  'NN',\n",
       "  'IN',\n",
       "  'NNP',\n",
       "  'NNP',\n",
       "  ',',\n",
       "  'DT',\n",
       "  'NNP',\n",
       "  'VBG',\n",
       "  'NN',\n",
       "  '.'])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penn_treebank[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../../data/train_50M_multimodal_clean/open_subtitles.train',\n",
       " '../../data/train_50M_multimodal_clean/childes.train',\n",
       " '../../data/train_50M_multimodal_clean/cc_3M_captions_reduced.train',\n",
       " '../../data/train_50M_multimodal_clean/bnc_spoken.train',\n",
       " '../../data/train_50M_multimodal_clean/gutenberg.train',\n",
       " '../../data/train_50M_multimodal_clean/simple_wiki.train',\n",
       " '../../data/train_50M_multimodal_clean/switchboard.train',\n",
       " '../../data/train_50M_multimodal_clean/local_narr_captions.train']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "data_dir = Path(\"../../data/train_50M_multimodal_clean/\")\n",
    "paths = [str(f) for f in data_dir.glob(\"*\") if f.is_file() and not f.name.endswith(\".DS_Store\") and f.suffix in [\".train\"]]\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File path:  ../../data/train_50M_multimodal_clean/switchboard.train\n",
      "Total lines in file:  95740\n",
      "Completed line 95740 out of 95740\r"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def process_text_file(file_path):\n",
    "    words = []\n",
    "    tags = []\n",
    "    f = open(file_path, 'rb')\n",
    "    n = sum(1 for _ in f)  # count the number of lines in the file\n",
    "    print(\"Total lines in file: \", n)\n",
    "    f.close()\n",
    "    pattern = r\"\\b\\w+(?:'\\w+)?\\b|\\b\\w+(?:-\\w+)*\\b|\\d+(?:\\.\\d+)?|\\S\"\n",
    "    k = 0\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            sentence = line.strip()\n",
    "\n",
    "            # Split the sentence using the refined regex pattern\n",
    "            tokens = re.findall(pattern, sentence)\n",
    "            tagged_sentence = pos_tag(tokens)\n",
    "            # print(tagged_sentence)\n",
    "            \n",
    "            for word, tag in tagged_sentence:\n",
    "                words.append(word)\n",
    "                tags.append(tag)\n",
    "            k += 1\n",
    "            \n",
    "            print(\"Completed line {0} out of {1}\".format(k, n), end=\"\\r\")\n",
    "    return list(zip(words, tags))\n",
    "\n",
    "# Example usage:\n",
    "global_list = []\n",
    "for file_path in paths[-2:-1]:\n",
    "    # file_path = paths[i]\n",
    "    print(\"File path: \", file_path)\n",
    "    result = process_text_file(file_path)\n",
    "    global_list.extend(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('A', 'DT'),\n",
       " (':', ':'),\n",
       " ('Okay', 'NN'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " (':', ':'),\n",
       " ('So', 'RB'),\n",
       " (',', ','),\n",
       " ('What', 'WP'),\n",
       " ('kind', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('experience', 'NN'),\n",
       " ('do', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " (',', ','),\n",
       " ('do', 'VBP'),\n",
       " ('you', 'PRP'),\n",
       " ('have', 'VB'),\n",
       " (',', ','),\n",
       " ('then', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('child', 'NN'),\n",
       " ('care', 'NN'),\n",
       " ('?', '.'),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('I', 'PRP'),\n",
       " ('guess', 'VBP'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('think', 'VBP'),\n",
       " (',', ','),\n",
       " ('uh', 'UH'),\n",
       " (',', ','),\n",
       " ('I', 'PRP'),\n",
       " ('wonder', 'VBP'),\n",
       " ('if', 'IN'),\n",
       " ('that', 'DT'),\n",
       " ('worked', 'VBD'),\n",
       " ('.', '.'),\n",
       " ('A', 'DT'),\n",
       " (':', ':'),\n",
       " ('Does', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('say', 'VB'),\n",
       " ('something', 'NN'),\n",
       " ('?', '.'),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('I', 'PRP'),\n",
       " ('think', 'VBP'),\n",
       " ('it', 'PRP'),\n",
       " ('usually', 'RB'),\n",
       " ('does', 'VBZ'),\n",
       " ('.', '.'),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('You', 'PRP'),\n",
       " ('might', 'MD'),\n",
       " ('try', 'VB'),\n",
       " (',', ','),\n",
       " ('uh', 'UH'),\n",
       " (',', ','),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('I', 'PRP'),\n",
       " (\"don't\", 'VBP'),\n",
       " ('know', 'VB'),\n",
       " (',', ','),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('hold', 'VB'),\n",
       " ('it', 'PRP'),\n",
       " ('down', 'RP'),\n",
       " ('a', 'DT'),\n",
       " ('little', 'JJ'),\n",
       " ('longer', 'JJR'),\n",
       " (',', ','),\n",
       " ('B', 'NN'),\n",
       " (':', ':'),\n",
       " ('and', 'CC'),\n",
       " ('see', 'VB'),\n",
       " ('if', 'IN'),\n",
       " ('it', 'PRP'),\n",
       " (',', ','),\n",
       " ('uh', 'UH'),\n",
       " (',', ','),\n",
       " ('A', 'DT'),\n",
       " (':', ':'),\n",
       " ('Okay', 'NN'),\n",
       " ('A', 'DT'),\n",
       " (':', ':'),\n",
       " ('Well', 'RB'),\n",
       " (',', ','),\n",
       " ('Does', 'VBZ'),\n",
       " ('it', 'PRP'),\n",
       " ('usually', 'RB'),\n",
       " ('make', 'VBP'),\n",
       " ('a', 'DT'),\n",
       " ('recording', 'NN')]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_list[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_list_set = set(global_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1070656"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(global_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store global list as a csv file\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(global_list_set, columns=[\"Word\", \"Tag\"])\n",
    "df.to_csv(\"pos_tagging_dataset.csv\", index=False, compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the global list to a file\n",
    "with open(\"../../data/train_50M_multimodal_clean/pos_tags_all_caption_and_text.txt\", \"w\") as file:\n",
    "    for word, tag in global_list:\n",
    "        file.write(\"{0} {1}\\n\".format(word, tag))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type '_io.TextIOWrapper' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type '_io.TextIOWrapper' has no len()"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'PRP'),\n",
       " ('am', 'VBP'),\n",
       " ('going', 'VBG'),\n",
       " ('to', 'TO'),\n",
       " ('school', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_tag(['I','am','going','to','school', '.'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refined regex pattern\n",
    "text = \"Here's an example sentence: with numbers 123 and punctuations, hyphens - and more! high-speed and it's 3.14 and example@example.com\"\n",
    "\n",
    "# Refined regex pattern\n",
    "pattern = r\"[A-Za-z0-9]+(?:'[A-Za-z]+)?|[A-Za-z]+(?:-[A-Za-z]+)*|[0-9]+(?:\\.[0-9]+)?|[^\\w\\s]\"\n",
    "\n",
    "# Split the sentence using the refined regex pattern\n",
    "tokens = re.findall(pattern, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [extract_features(tokens, i) for i in range(len(tokens))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word': '.',\n",
       " 'is_first': False,\n",
       " 'is_last': False,\n",
       " 'is_capitalized': True,\n",
       " 'is_all_caps': True,\n",
       " 'is_all_lower': True,\n",
       " 'is_alphanumeric': 0,\n",
       " 'prefix-1': '.',\n",
       " 'prefix-2': '.',\n",
       " 'prefix-3': '.',\n",
       " 'suffix-1': '.',\n",
       " 'suffix-2': '.',\n",
       " 'suffix-3': '.',\n",
       " 'prev_word': 'example',\n",
       " 'next_word': '',\n",
       " 'has_hyphen': False,\n",
       " 'is_numeric': False,\n",
       " 'capitals_inside': False,\n",
       " 'punctuation': True}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features[-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Here's\",\n",
       " 'an',\n",
       " 'example',\n",
       " 'sentence',\n",
       " ':',\n",
       " 'with',\n",
       " 'numbers',\n",
       " '123',\n",
       " 'and',\n",
       " 'punctuations',\n",
       " ',',\n",
       " 'hyphens',\n",
       " '-',\n",
       " 'and',\n",
       " 'more',\n",
       " '!',\n",
       " 'high',\n",
       " '-',\n",
       " 'speed',\n",
       " 'and',\n",
       " \"it's\",\n",
       " '3',\n",
       " '.',\n",
       " '14',\n",
       " 'and',\n",
       " 'example',\n",
       " '@',\n",
       " 'example',\n",
       " '.',\n",
       " 'com']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load an expertly annotated dataset.\n",
    "2. Use that model to annotate the custom dataset.\n",
    "3. Train a model on the custom dataset.\n",
    "4. Use that model for the downstream task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spacy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import libraries.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mspacy\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spacy'"
     ]
    }
   ],
   "source": [
    "# import libraries.\n",
    "import nltk\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load an expertly annotated dataset.\n",
    "# Load the Penn Treebank dataset.\n",
    "nltk.download('treebank')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
    "sent = \"The tagger produced good results\"\n",
    "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
    "\n",
    "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
    "penn_results = penn_crf.predict_single(features)\n",
    "ud_results = ud_crf.predict_single(features)\n",
    "\n",
    "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
    "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
    "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
    "\n",
    "#The results come out here! Notice the difference in tags.\n",
    "print(penn_tups)\n",
    "print(ud_tups)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

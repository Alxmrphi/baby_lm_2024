program: src/train_git_base_osf_text.py
name: text_only_training
method: grid
metric:
  goal: minimize
  name: val_loss
parameters:
  lr:
    values: [1e-5]
  batch_size:
    values: [256]
  n_epochs:
    values: [20]
  min_save_every:
    values: [1]
  n_workers:
    values: [20]
  optimizer:
    values: ["adam"]
  dataset_size:
    values: [-1]
  seed:
    values: [0, 1, 2]
  do_curriculum:
    values: [False]  # This is False for standard-finetuning.
  model_name:
    values: ['flamingo']  # Can be 'flamingo' or 'git'.
  model_type:
    values: ['causal_lm']  # Can be 'causal_lm' or 'sequence'.
  max_token_length:
    values: [50]
  initialize_with_text:
    values: [False]
  use_accelerate:
    values: [False]
  fp16:
    values: [True]
  tokenizer_path:
    values: ['./src/tokenizer/hf_wordpiece_tokenizer_from_bert-base-uncased/']
    # TODO: See if the same tokenizer can be used for Flamingo.
  text_init_model_path:
    values: [None]
  load_optimizer:
    values: [False]
  train_on_full_data:
    values: [True]